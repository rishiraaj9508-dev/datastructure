{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-s68m-a3IAp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What does R-squared represent in a regression model?\n",
        "R-squared (R¬≤) measures the proportion of variance in the dependent variable that is predictable from the independent variable(s). It ranges from 0 to 1. A higher value indicates a better fit.\n",
        "\n",
        "2. What are the assumptions of linear regression?\n",
        "\n",
        "Linearity: Relationship between dependent and independent variables is linear.\n",
        "\n",
        "Independence: Residuals are independent.\n",
        "\n",
        "Homoscedasticity: Constant variance of residuals.\n",
        "\n",
        "Normality: Residuals are normally distributed.\n",
        "\n",
        "No multicollinearity: Independent variables are not highly correlated.\n",
        "\n",
        "3. What is the difference between R-squared and Adjusted R-squared?\n",
        "\n",
        "R¬≤: Measures the goodness of fit but increases with more features, even irrelevant ones.\n",
        "\n",
        "Adjusted R¬≤: Adjusts for the number of predictors; increases only if the new predictor improves the model.\n",
        "\n",
        "4. Why do we use Mean Squared Error (MSE)?\n",
        "MSE quantifies the average squared difference between predicted and actual values, penalizing larger errors more heavily.\n",
        "\n",
        "5. What does an Adjusted R-squared value of 0.85 indicate?\n",
        "It indicates that 85% of the variance in the dependent variable is explained by the model, adjusted for the number of predictors.\n",
        "\n",
        "6. How do we check for normality of residuals in linear regression?\n",
        "\n",
        "Histogram of residuals\n",
        "\n",
        "Q-Q plot\n",
        "\n",
        "Shapiro-Wilk or Kolmogorov-Smirnov tests\n",
        "\n",
        "7. What is multicollinearity, and how does it impact regression?\n",
        "Multicollinearity occurs when independent variables are highly correlated. It inflates the standard errors of coefficients and reduces model interpretability.\n",
        "\n",
        "8. What is Mean Absolute Error (MAE)?\n",
        "MAE is the average absolute difference between predicted and actual values. Less sensitive to outliers than MSE.\n",
        "\n",
        "9. What are the benefits of using an ML pipeline?\n",
        "\n",
        "Automates preprocessing and modeling\n",
        "\n",
        "Reduces human error\n",
        "\n",
        "Ensures reproducibility\n",
        "\n",
        "Facilitates hyperparameter tuning\n",
        "\n",
        "10. Why is RMSE considered more interpretable than MSE?\n",
        "RMSE is in the same units as the target variable, making it easier to understand.\n",
        "\n",
        "11. What is pickling in Python, and how is it useful in ML?\n",
        "Pickling serializes Python objects (like trained ML models) into files to save and load later.\n",
        "\n",
        "12. What does a high R-squared value mean?\n",
        "The model explains a large proportion of the variance in the dependent variable.\n",
        "\n",
        "13. What happens if linear regression assumptions are violated?\n",
        "\n",
        "Biased or inefficient estimates\n",
        "\n",
        "Incorrect significance tests\n",
        "\n",
        "Poor predictive performance\n",
        "\n",
        "14. How can we address multicollinearity in regression?\n",
        "\n",
        "Remove highly correlated features\n",
        "\n",
        "Use PCA or dimensionality reduction\n",
        "\n",
        "Apply regularization (Ridge or Lasso)\n",
        "\n",
        "15. How can feature selection improve model performance in regression analysis?\n",
        "\n",
        "Reduces overfitting\n",
        "\n",
        "Simplifies the model\n",
        "\n",
        "Improves interpretability and computation efficiency\n",
        "\n",
        "16. How is Adjusted R-squared calculated?\n",
        "\n",
        "ùê¥\n",
        "ùëë\n",
        "ùëó\n",
        "ùë¢\n",
        "ùë†\n",
        "ùë°\n",
        "ùëí\n",
        "ùëë\n",
        "\n",
        "ùëÖ\n",
        "2\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùëÖ\n",
        "2\n",
        ")\n",
        "(\n",
        "ùëõ\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "ùëõ\n",
        "‚àí\n",
        "ùëù\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "Adjusted R\n",
        "2\n",
        "=1‚àí(\n",
        "n‚àíp‚àí1\n",
        "(1‚àíR\n",
        "2\n",
        ")(n‚àí1)\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "Where n = number of observations, p = number of predictors.\n",
        "\n",
        "17. Why is MSE sensitive to outliers?\n",
        "Because errors are squared, large errors disproportionately affect the MSE.\n",
        "\n",
        "18. What is the role of homoscedasticity in linear regression?\n",
        "Ensures that residuals have constant variance; violation can lead to inefficient estimates.\n",
        "\n",
        "19. What is Root Mean Squared Error (RMSE)?\n",
        "RMSE = ‚àöMSE, representing the average magnitude of errors in the same unit as the target variable.\n",
        "\n",
        "20. Why is pickling considered risky?\n",
        "Pickled files can execute arbitrary code if tampered with, posing security risks.\n",
        "\n",
        "21. What alternatives exist to pickling for saving ML models?\n",
        "\n",
        "Joblib\n",
        "\n",
        "ONNX\n",
        "\n",
        "HDF5 / TensorFlow SavedModel\n",
        "\n",
        "PMML\n",
        "\n",
        "22. What is heteroscedasticity, and why is it a problem?\n",
        "Variance of residuals changes with predictors; leads to inefficient and biased estimates.\n",
        "\n",
        "23. How can interaction terms enhance a regression model's predictive power?\n",
        "They allow the model to capture effects of variable combinations that are not additive."
      ],
      "metadata": {
        "id": "9du6pBDSt-Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "X = diamonds[['carat']]  # Feature\n",
        "y = diamonds['price']    # Target\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "residuals = y - model.predict(X)\n",
        "\n",
        "# Plot residuals\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title(\"Distribution of Residuals\")\n",
        "plt.show()\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "X = diamonds[['carat']]  # Feature\n",
        "y = diamonds['price']    # Target\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "residuals = y - model.predict(X)\n",
        "\n",
        "# Plot residuals\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title(\"Distribution of Residuals\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jfStmeRruiOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Synthetic data\n",
        "X = np.random.rand(50,1)*10\n",
        "y = 2*X + 5 + np.random.randn(50,1)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, label=\"Data\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Regression Line\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tkoZcK1nu3vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.random.rand(50,1)\n",
        "y = 3*X + np.random.randn(50,1)\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "\n",
        "# Pickle model\n",
        "with open('linear_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n"
      ],
      "metadata": {
        "id": "GAxt6kMgu3yb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.arange(0,10,0.5).reshape(-1,1)\n",
        "y = 0.5*X**2 + X + 2 + np.random.randn(len(X),1)\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, model.predict(X_poly), color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N-B2QYCHu31k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.random.rand(100,1)*10\n",
        "y = 4*X + 10 + np.random.randn(100,1)\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"Coefficient:\", model.coef_[0])\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "id": "z4jjF9SEu34v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X = np.arange(0,10,0.5).reshape(-1,1)\n",
        "y = 2*X**2 + 3*X + 5 + np.random.randn(len(X),1)\n",
        "\n",
        "for degree in [1,2,3]:\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression().fit(X_poly, y)\n",
        "    y_pred = model.predict(X_poly)\n",
        "    print(f\"Degree {degree} R-squared:\", r2_score(y, y_pred))\n"
      ],
      "metadata": {
        "id": "MJdjistru37e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.random.rand(50,2)\n",
        "y = 3*X[:,0] + 5*X[:,1] + np.random.randn(50)\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"R-squared:\", model.score(X, y))\n"
      ],
      "metadata": {
        "id": "3h68jtNzu3-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.random.rand(50,1)*10\n",
        "y = 5*X + 2 + np.random.randn(50,1)\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, model.predict(X), color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d6lYccQ4u4Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "X = pd.DataFrame(np.random.rand(100,3), columns=['X1','X2','X3'])\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = X.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "WCxIafxWu4Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.linspace(0,5,50).reshape(-1,1)\n",
        "y = X**4 - 2*X**3 + X**2 + np.random.randn(50,1)\n",
        "\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, model.predict(X_poly), color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QKrv-U6su4Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.random.rand(100,3)\n",
        "y = 2*X[:,0] + 3*X[:,1] + 5*X[:,2] + np.random.randn(100)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regression', LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline.fit(X, y)\n",
        "print(\"R-squared:\", pipeline.score(X, y))\n"
      ],
      "metadata": {
        "id": "k29TpvBju4Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.linspace(0,5,50).reshape(-1,1)\n",
        "y = X**3 + 2*X**2 + X + np.random.randn(50,1)\n",
        "\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, model.predict(X_poly), color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "91m9m2Gau4NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.random.rand(100,5)\n",
        "y = 2*X[:,0] + 3*X[:,1] + X[:,2] + 0.5*X[:,3] + 4*X[:,4] + np.random.randn(100)\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"R-squared:\", model.score(X, y))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "id": "tz6BH8rau4QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.random.rand(50,1)*10\n",
        "y = 3*X + 5 + np.random.randn(50,1)\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, model.predict(X), color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fOFVVWWMu4TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "y_true = [3, -0.5, 2, 7]\n",
        "y_pred = [2.5, 0.0, 2, 8]\n",
        "\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)\n"
      ],
      "metadata": {
        "id": "geNnSSJ1ufTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Synthetic dataset\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 2)\n",
        "y = 3*X[:,0] + 2*X[:,1] + np.random.randn(100)\n",
        "\n",
        "# Linearity\n",
        "plt.scatter(X[:,0], y)\n",
        "plt.title(\"Linearity Check\")\n",
        "plt.show()\n",
        "\n",
        "# Residuals for homoscedasticity\n",
        "model = LinearRegression().fit(X, y)\n",
        "residuals = y - model.predict(X)\n",
        "plt.scatter(model.predict(X), residuals)\n",
        "plt.title(\"Residuals vs Predicted\")\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "# Multicollinearity (correlation matrix)\n",
        "df = pd.DataFrame(X, columns=['X1','X2'])\n",
        "print(\"Correlation matrix:\\n\", df.corr())\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Synthetic dataset\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 2)\n",
        "y = 3*X[:,0] + 2*X[:,1] + np.random.randn(100)\n",
        "\n",
        "# Linearity\n",
        "plt.scatter(X[:,0], y)\n",
        "plt.title(\"Linearity Check\")\n",
        "plt.show()\n",
        "\n",
        "# Residuals for homoscedasticity\n",
        "model = LinearRegression().fit(X, y)\n",
        "residuals = y - model.predict(X)\n",
        "plt.scatter(model.predict(X), residuals)\n",
        "plt.title(\"Residuals vs Predicted\")\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "# Multicollinearity (correlation matrix)\n",
        "df = pd.DataFrame(X, columns=['X1','X2'])\n",
        "print(\"Correlation matrix:\\n\", df.corr())\n"
      ],
      "metadata": {
        "id": "Ci20ynFtubD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oPwF0VnHuc06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "\n",
        "X = np.random.rand(100,3)\n",
        "y = X[:,0]*2 + X[:,1]*3 + np.random.randn(100)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "iOVWIqwGuYwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1],[2],[3],[4],[5]])\n",
        "y = np.array([2,4,5,4,5])\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"R-squared:\", model.score(X, y))\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1],[2],[3],[4],[5]])\n",
        "y = np.array([2,4,5,4,5])\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"R-squared:\", model.score(X, y))\n"
      ],
      "metadata": {
        "id": "9VzTxcC2uXMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "tips = sns.load_dataset('tips')\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "sns.scatterplot(x='total_bill', y='tip', data=tips)\n",
        "plt.plot(X, model.predict(X), color='red')\n",
        "plt.show()\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "tips = sns.load_dataset('tips')\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "sns.scatterplot(x='total_bill', y='tip', data=tips)\n",
        "plt.plot(X, model.predict(X), color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "20yi1DKDuKd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_2GkbZ-LuVMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.random.rand(100,3)\n",
        "y = 2*X[:,0] + 4*X[:,1] + 3*X[:,2] + np.random.randn(100)\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"R-squared:\", model.score(X, y))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "id": "aIH-vPD5vedp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.random.rand(50,1)\n",
        "y = 2*X + np.random.randn(50,1)\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(model, 'linear_model_joblib.pkl')\n",
        "\n",
        "# Load model\n",
        "loaded_model = joblib.load('linear_model_joblib.pkl')\n",
        "print(\"Predicted:\", loaded_model.predict([[1.5]]))\n"
      ],
      "metadata": {
        "id": "4u9xW6kxvf9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "tips = sns.load_dataset('tips')\n",
        "X = pd.get_dummies(tips[['sex','smoker','day','time']], drop_first=True)\n",
        "y = tips['tip']\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "id": "lZE5CSEpvi4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "\n",
        "X = np.random.rand(50,2)\n",
        "y = 4*X[:,0] + 2*X[:,1] + np.random.randn(50)\n",
        "\n",
        "lr = LinearRegression().fit(X, y)\n",
        "ridge = Ridge(alpha=1.0).fit(X, y)\n",
        "\n",
        "print(\"Linear Regression Coefficients:\", lr.coef_)\n",
        "print(\"R-squared:\", lr.score(X, y))\n",
        "\n",
        "print(\"Ridge Regression Coefficients:\", ridge.coef_)\n",
        "print(\"R-squared:\", ridge.score(X, y))\n"
      ],
      "metadata": {
        "id": "n3ijmJ68vlX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X = np.random.rand(100,2)\n",
        "y = 3*X[:,0] + 5*X[:,1] + np.random.randn(100)\n",
        "\n",
        "model = LinearRegression()\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "print(\"Cross-validated R2 scores:\", scores)\n",
        "print(\"Mean R2:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "qMKWJACgvnWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X = np.linspace(0,5,50).reshape(-1,1)\n",
        "y = 0.5*X**3 + X**2 + X + np.random.randn(50,1)\n",
        "\n",
        "for degree in [1,2,3]:\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression().fit(X_poly, y)\n",
        "    y_pred = model.predict(X_poly)\n",
        "    print(f\"Degree {degree} R-squared:\", r2_score(y, y_pred))\n"
      ],
      "metadata": {
        "id": "PDpzUkXjvpz_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}